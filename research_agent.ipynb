{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PFp0JhOWCU5"
      },
      "source": [
        "# GPT-4o Research Agent in LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EckNKuhXWHEJ"
      },
      "source": [
        "**Research agents** are multi-step LLM agents that through multiple steps can produce in depth research reports on a topic of our choosing. Most research agents are packed up into their own frameworks, like BlockAGI and others.\n",
        "\n",
        "In this example, we want to demonstrate how we can build our own AI research agent using `gpt-4o`, Pinecone, LangGraph, arXiv, and Google via the SerpAPI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCVLlfNyAtxD",
        "outputId": "658c89aa-52b6-4663-cfb7-1391d2a1170d"
      },
      "outputs": [],
      "source": [
        "!apt-get install graphviz libgraphviz-dev pkg-config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3hzHJ4NoLYD"
      },
      "source": [
        "We need these prerequisite libraries to run a graph visualization library (`pygraphviz`). We will use this library during this notebook to understand the structure of our graphs _but_ it is not required to use `langgraph`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbR8rt-EtqWh"
      },
      "source": [
        "Now we install Python libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUt2EoJZu6M3"
      },
      "outputs": [],
      "source": [
        "!pip install \\\n",
        "    datasets \\\n",
        "    langchain-pinecone\\\n",
        "    langchain-openai \\\n",
        "    langchain \\\n",
        "    langchain-core \\\n",
        "    langgraph \\\n",
        "    semantic-router \\\n",
        "    serpapi \\\n",
        "    google-search-results \\\n",
        "    sentence-transformers\\\n",
        "    # pygraphviz==1.12  # for visualizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bABCOVLFdrW4"
      },
      "source": [
        "## Research Agent Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t1rZK3FdtLh"
      },
      "source": [
        "Our research agent will consist of a function calling AI agent that has access to several tools that it can use to find information on a particular topic. It will be able to use several tools over multiple steps, meaning it can find information on one topic, broaden the scope of knowledge on this topic and _even_ investigate parallel topics where relevant.\n",
        "\n",
        "The tools we will be using are:\n",
        "\n",
        "* **ArXiv paper fetch**: Given an arXiv paper ID, this tool provides our agent with the abstract of the paper.\n",
        "* **Web search**: This tool provides our agent with access to Google search for more generalized queries.\n",
        "* **RAG search**: We will create a knowledge base containing AI arXiv papers. This tool provides our agent with access to this knowledge.\n",
        "* **RAG search with filter**: Sometimes our agent may need more information from a specific paper, this tool allows our agent to do just that.\n",
        "* **Final answer**: We create a custom final answer tool that forces our agent to output information in a specific format like:\n",
        "\n",
        "```\n",
        "INTRODUCTION\n",
        "------------\n",
        "<some intro to our report>\n",
        "\n",
        "RESEARCH STEPS\n",
        "--------------\n",
        "<the steps the agent took during research>\n",
        "\n",
        "REPORT\n",
        "------\n",
        "<the report main body>\n",
        "\n",
        "CONCLUSION\n",
        "----------\n",
        "<the report conclusion>\n",
        "\n",
        "SOURCES\n",
        "-------\n",
        "<any sources the agent used>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vovHYoM-AdbU"
      },
      "source": [
        "## Setup Knowledge Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEtKvy4_AgLy"
      },
      "source": [
        "We'll be running our agent against a knowledge base — which requires a Pinecone index to be built.\n",
        "\n",
        "You can, if needed, skip this step and replace the `search` tool with a placeholder value if wanting to quickly test the structure of a RAG agent _without_ the RAG.\n",
        "\n",
        "If you want full functionality here, you do need to run this section — but we'll make it quick."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohBxmFyTBFT0"
      },
      "source": [
        "### Download a Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1ufvA8hBKHx"
      },
      "source": [
        "The first thing we need for an agent using RAG is somewhere we want to pull knowledge from. We will use v2 of the AI ArXiv dataset, available on Hugging Face Datasets at [`jamescalam/ai-arxiv2-chunks`](https://huggingface.co/datasets/jamescalam/ai-arxiv2-chunks).\n",
        "\n",
        "Note: we're using the prechunked dataset. For the raw version see [`jamescalam/ai-arxiv2`](https://huggingface.co/datasets/jamescalam/ai-arxiv2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehcjNQ49BBuI",
        "outputId": "9b05f125-f2cd-4cf0-b6fd-a84d5a73b974"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"jamescalam/ai-arxiv2-semantic-chunks\", split=\"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pElKCcVSSddB",
        "outputId": "91378db1-cec5-4d65-d872-bdd6e77f822a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '2401.04088#0',\n",
              " 'title': 'Mixtral of Experts',\n",
              " 'content': '4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.',\n",
              " 'prechunk_id': '',\n",
              " 'postchunk_id': '2401.04088#1',\n",
              " 'arxiv_id': '2401.04088',\n",
              " 'references': ['1905.07830']}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJjZMEO_HmOi"
      },
      "source": [
        "Building a knowledge base:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjsIFVbtKBu4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from semantic_router.encoders import OpenAIEncoder\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\"OpenAI API key: \")\n",
        "\n",
        "encoder = OpenAIEncoder(name=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J233t2I4HmjD"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Pinecone API key: \")\n",
        "\n",
        "# configure client\n",
        "pc = Pinecone(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "actWflaEKgVb"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-west-2\"  # us-east-1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWijfk8LKil7",
        "outputId": "44740df7-25b2-455f-9856-bd24e8f11b1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dims = len(encoder([\"some random text\"])[0])\n",
        "dims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-GHzWs3KlJn",
        "outputId": "8636719c-ae83-451b-e230-f2bb209d67a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 10000}},\n",
              " 'total_vector_count': 10000}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "index_name = \"gpt-4o-research-agent\"\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=dims,  # dimensionality of embed 3\n",
        "        metric='dotproduct',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "time.sleep(1)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwNPeBbZK0hx"
      },
      "source": [
        "Populate our knowledge base:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "925f65b6b0cc47be803c021f644a8f84",
            "24a571b8cc864f60bea7ae90ef331b21",
            "45580486ce0742b29d0de17850c42060",
            "f1f07bc63a234590ac229d63217e197a",
            "a6d67d04d1c8432c8840f411027a0e79",
            "c52a55d3643b4f1c83bdf1b1b5fce242",
            "7de73bce9c0540e2b3b2c599d57ae215",
            "84c25583658a4f4caeedbc19dd29457a",
            "a9594c4de9734895862a48f90bc8c621",
            "48ec484f11324501b1020314d4989da6",
            "bfecdc3138334cd1adcd055f338060d1"
          ]
        },
        "id": "5iQ2YJPeK2Us",
        "outputId": "823fcf4f-3735-4a85-e041-0b87df7bdb56"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "925f65b6b0cc47be803c021f644a8f84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/79 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# easier to work with dataset as pandas dataframe\n",
        "data = dataset.to_pandas().iloc[:10000]\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    batch = data[i:i_end].to_dict(orient=\"records\")\n",
        "    # get batch of data\n",
        "    metadata = [{\n",
        "        \"title\": r[\"title\"],\n",
        "        \"content\": r[\"content\"],\n",
        "        \"arxiv_id\": r[\"arxiv_id\"],\n",
        "        \"references\": r[\"references\"].tolist()\n",
        "    } for r in batch]\n",
        "    # generate unique ids for each chunk\n",
        "    ids = [r[\"id\"] for r in batch]\n",
        "    # get text content to embed\n",
        "    content = [r[\"content\"] for r in batch]\n",
        "    # embed text\n",
        "    embeds = encoder(content)\n",
        "    # add to Pinecone\n",
        "    index.upsert(vectors=zip(ids, embeds, metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=HuggingFaceEmbeddings(),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%export` not found.\n"
          ]
        }
      ],
      "source": [
        "%export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import DataFrameLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
        "\n",
        "\n",
        "# Load dataset as pandas DataFrame\n",
        "df = dataset.to_pandas().iloc[:500]\n",
        "# Convert each column to string individually\n",
        "df['id'] = df['id'].astype(str)\n",
        "df['title'] = df['title'].astype(str)\n",
        "df['content'] = df['content'].astype(str)\n",
        "df['prechunk_id'] = df['prechunk_id'].astype(str)\n",
        "df['postchunk_id'] = df['postchunk_id'].astype(str)\n",
        "df['arxiv_id'] = df['arxiv_id'].astype(str)\n",
        "df['references'] = df['references'].astype(str)\n",
        "\n",
        "loader = DataFrameLoader(data, page_content_column=\"content\")\n",
        "documents = loader.load()\n",
        "\n",
        "\n",
        "# Split the documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=0)\n",
        "doc_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=HuggingFaceEmbeddings(),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   id            500 non-null    object\n",
            " 1   title         500 non-null    object\n",
            " 2   content       500 non-null    object\n",
            " 3   prechunk_id   500 non-null    object\n",
            " 4   postchunk_id  500 non-null    object\n",
            " 5   arxiv_id      500 non-null    object\n",
            " 6   references    500 non-null    object\n",
            "dtypes: object(7)\n",
            "memory usage: 27.5+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hITmdwQ7Qns0"
      },
      "source": [
        "## Graph State\n",
        "\n",
        "We will define a custom graph state to support our agent-oriented decision making."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7Fj9KNjQvUq"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, List, Union\n",
        "from langchain_core.agents import AgentAction, AgentFinish\n",
        "from langchain_core.messages import BaseMessage\n",
        "import operator\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    input: str\n",
        "    chat_history: list[BaseMessage]\n",
        "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp4uQFBKQdyI"
      },
      "source": [
        "There are four parts to our agent state, those are:\n",
        "\n",
        "* `input`: this is the user's most recent query, usually this would be a question that we want to answer with our research agent.\n",
        "* `chat_history`: we are building a conversational agent that can support multiple interactions, to allow previous interactions to provide additional context throughout our agent logic we include the chat history in the agent state.\n",
        "* `intermediate_steps`: provides a record of all steps the research agent will take between the user asking a question via `input` and the agent providing a final answer. This can include things like \"search arxiv\", \"perform general purpose web search\", etc. These intermediate steps are crucial to allowing the agent to follow a path of coherent actions and ultimately producing an informed final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlVbgX39Rhxj"
      },
      "source": [
        "## Custom Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io-lnbpgKHzy"
      },
      "source": [
        "We will define several tools for this agent that will focus on initial data discovery, that will allow the LLM to use more tools to research more deeply via a variety of different routes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9MYnZvtF9WN"
      },
      "source": [
        "### ArXiv Paper Fetch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQZjy5TZGAFB"
      },
      "source": [
        "The `fetch_arxiv` tool will allow our agent to get the summary of a specific paper given an ArXiv paper ID. To do this, we will simply send a GET request to arXiv and use regex to extract the paper abstract."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "B2W-V1cOGzwJ",
        "outputId": "37a39e8c-491c-448f-f0ba-86818e8353ca"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# we will test with the mixtral paper\n",
        "arxiv_id = \"2401.04088\"\n",
        "\n",
        "res = requests.get(\n",
        "    f\"https://export.arxiv.org/abs/{arxiv_id}\"\n",
        ")\n",
        "res.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPbxToG5HDm_"
      },
      "source": [
        "There's a lot going on there. Fortunately, we can use some _relatively_ straightforward regex to find the paper abstract."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efGqEsaAF26E",
        "outputId": "e7b050f6-1104-41aa-8087-416638c6b7bf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# our regex\n",
        "abstract_pattern = re.compile(\n",
        "    r'<blockquote class=\"abstract mathjax\">\\s*<span class=\"descriptor\">Abstract:</span>\\s*(.*?)\\s*</blockquote>',\n",
        "    re.DOTALL\n",
        ")\n",
        "\n",
        "# we search\n",
        "re_match = abstract_pattern.search(res.text)\n",
        "\n",
        "# and now let's see what we got\n",
        "print(re_match.group(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vjd5FhzHpqJ"
      },
      "source": [
        "Now we pack all of this logic into a tool for our agent to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpYR2DYsHtgb"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool(\"fetch_arxiv\")\n",
        "def fetch_arxiv(arxiv_id: str):\n",
        "    \"\"\"Gets the abstract from an ArXiv paper given the arxiv ID. Useful for\n",
        "    finding high-level context about a specific paper.\"\"\"\n",
        "    # get paper page in html\n",
        "    res = requests.get(\n",
        "        f\"https://export.arxiv.org/abs/{arxiv_id}\"\n",
        "    )\n",
        "    # search html for abstract\n",
        "    re_match = abstract_pattern.search(res.text)\n",
        "    # return abstract text\n",
        "    return re_match.group(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uydhqzKIU2F"
      },
      "source": [
        "Let's test the tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeEnI3V7IVDe",
        "outputId": "b2d50557-a82e-49e3-e5a6-56dabd421a92"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    fetch_arxiv.invoke(input={\"arxiv_id\": arxiv_id})\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLX1N7oWKiNZ"
      },
      "source": [
        "### Web Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsOutTz9KjoL"
      },
      "source": [
        "The web search tool will provide the agent with access to web search. It will be instructed to use this for more general knowledge queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuv_exwsH8ZJ"
      },
      "outputs": [],
      "source": [
        "from serpapi import GoogleSearch\n",
        "\n",
        "serpapi_params = {\n",
        "    \"engine\": \"google\",\n",
        "    \"api_key\": os.getenv(\"SERPAPI_KEY\") or getpass(\"SerpAPI key: \")\n",
        "}\n",
        "\n",
        "search = GoogleSearch({\n",
        "    **serpapi_params,\n",
        "    \"q\": \"coffee\"\n",
        "})\n",
        "\n",
        "results = search.get_dict()[\"organic_results\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KklJP-XSTqLG"
      },
      "outputs": [],
      "source": [
        "contexts = \"\\n---\\n\".join(\n",
        "    [\"\\n\".join([x[\"title\"], x[\"snippet\"], x[\"link\"]]) for x in results]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b083gU3T-8K",
        "outputId": "830662a7-f719-4678-8ea5-85c86d4d5b24"
      },
      "outputs": [],
      "source": [
        "print(contexts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtmOt4nQH4KS"
      },
      "source": [
        "We put this process into a tool:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYmOL54xH6GH"
      },
      "outputs": [],
      "source": [
        "@tool(\"web_search\")\n",
        "def web_search(query: str):\n",
        "    \"\"\"Finds general knowledge information using Google search. Can also be used\n",
        "    to augment more 'general' knowledge to a previous specialist query.\"\"\"\n",
        "    search = GoogleSearch({\n",
        "        **serpapi_params,\n",
        "        \"q\": query,\n",
        "        \"num\": 5\n",
        "    })\n",
        "    results = search.get_dict()[\"organic_results\"]\n",
        "    contexts = \"\\n---\\n\".join(\n",
        "        [\"\\n\".join([x[\"title\"], x[\"snippet\"], x[\"link\"]]) for x in results]\n",
        "    )\n",
        "    return contexts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCBZjWfAIN0N"
      },
      "source": [
        "### RAG Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY37AFP6RBOP"
      },
      "source": [
        "We provide two RAG-focused tools for our agent. The `rag_search` allows the agent to perform a simple RAG search for some information across _all_ indexed research papers. The `rag_search_filter` also searches, but _within_ a specific paper which is filtered for via the `arxiv_id` parameter.\n",
        "\n",
        "We also define the `format_rag_contexts` function to handle the transformation of our Pinecone results from a JSON object to a readble plaintext format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPcrbQfdRrda"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "def format_rag_contexts(matches: list):\n",
        "    contexts = []\n",
        "    for x in matches:\n",
        "        text = (\n",
        "            f\"Title: {x['metadata']['title']}\\n\"\n",
        "            f\"Content: {x['metadata']['content']}\\n\"\n",
        "            f\"ArXiv ID: {x['metadata']['arxiv_id']}\\n\"\n",
        "            f\"Related Papers: {x['metadata']['references']}\\n\"\n",
        "        )\n",
        "        contexts.append(text)\n",
        "    context_str = \"\\n---\\n\".join(contexts)\n",
        "    return context_str\n",
        "\n",
        "@tool(\"rag_search_filter\")\n",
        "def rag_search_filter(query: str, arxiv_id: str):\n",
        "    \"\"\"Finds information from our ArXiv database using a natural language query\n",
        "    and a specific ArXiv ID. Allows us to learn more details about a specific paper.\"\"\"\n",
        "    xq = encoder([query])\n",
        "    xc = index.query(vector=xq, top_k=6, include_metadata=True, filter={\"arxiv_id\": arxiv_id})\n",
        "    context_str = format_rag_contexts(xc[\"matches\"])\n",
        "    return context_str\n",
        "\n",
        "@tool(\"rag_search\")\n",
        "def rag_search(query: str):\n",
        "    \"\"\"Finds specialist information on AI using a natural language query.\"\"\"\n",
        "    xq = encoder([query])\n",
        "    xc = index.query(vector=xq, top_k=2, include_metadata=True)\n",
        "    context_str = format_rag_contexts(xc[\"matches\"])\n",
        "    return context_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDBmpJ0URzKS"
      },
      "source": [
        "### Final Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWVKYuQ6R6nB"
      },
      "source": [
        "Finally, we define a \"final answer\" tool. This isn't a tool in the usual sense, instead we use it to force a particular output format from our LLM via the function/tool calling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIjAlcHdfJCG"
      },
      "outputs": [],
      "source": [
        "@tool(\"final_answer\")\n",
        "def final_answer(\n",
        "    introduction: str,\n",
        "    research_steps: str,\n",
        "    main_body: str,\n",
        "    conclusion: str,\n",
        "    sources: str\n",
        "):\n",
        "    \"\"\"Returns a natural language response to the user in the form of a research\n",
        "    report. There are several sections to this report, those are:\n",
        "    - `introduction`: a short paragraph introducing the user's question and the\n",
        "    topic we are researching.\n",
        "    - `research_steps`: a few bullet points explaining the steps that were taken\n",
        "    to research your report.\n",
        "    - `main_body`: this is where the bulk of high quality and concise\n",
        "    information that answers the user's question belongs. It is 3-4 paragraphs\n",
        "    long in length.\n",
        "    - `conclusion`: this is a short single paragraph conclusion providing a\n",
        "    concise but sophisticated view on what was found.\n",
        "    - `sources`: a bulletpoint list provided detailed sources for all information\n",
        "    referenced during the research process\n",
        "    \"\"\"\n",
        "    if type(research_steps) is list:\n",
        "        research_steps = \"\\n\".join([f\"- {r}\" for r in research_steps])\n",
        "    if type(sources) is list:\n",
        "        sources = \"\\n\".join([f\"- {s}\" for s in sources])\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8OsQ3tIS-_t"
      },
      "source": [
        "## Initialize the \"Oracle\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YykLRD5rXfr5"
      },
      "source": [
        "The **Oracle** LLM is our graph's decision maker. It decides which path we should take down our graph. It functions similarly to an agent but is much simpler and reliable.\n",
        "\n",
        "The Oracle consists of an LLM provided with a set of potential function calls (ie our tools) that it can decide to use — we force it to use _at least_ one of those tool using the `tool_choice=\"any\"` setting (see below). Our Oracle only makes the decision to use a tool, it doesn't execute the tool code itself (we do that seperately in our graph)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRlREI1wYSd0"
      },
      "source": [
        "### Oracle Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFE638_CYUbf"
      },
      "source": [
        "Our prompt for the Oracle will emphasize it's decision making ability within the `system_prompt`, leave a placeholder for us to later insert `chat_history`, and provide a place for us to insert the user `input`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBZrvHDAYmOP"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "system_prompt = \"\"\"You are the oracle, the great AI decision maker.\n",
        "Given the user's query you must decide what to do with it based on the\n",
        "list of tools provided to you.\n",
        "\n",
        "If you see that a tool has been used (in the scratchpad) with a particular\n",
        "query, do NOT use that same tool with the same query again. Also, do NOT use\n",
        "any tool more than twice (ie, if the tool appears in the scratchpad twice, do\n",
        "not use it again).\n",
        "\n",
        "You should aim to collect information from a diverse range of sources before\n",
        "providing the answer to the user. Once you have collected plenty of information\n",
        "to answer the user's question (stored in the scratchpad) use the final_answer\n",
        "tool.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"assistant\", \"scratchpad: {scratchpad}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euGh0137Ynxv"
      },
      "source": [
        "Next, we must initialize our `llm` (for this we use `gpt-4o`) and then create the _runnable_ pipeline of our Oracle.\n",
        "\n",
        "The runnable connects our inputs (the user `input` and `chat_history`) to our `prompt`, and our `prompt` to our `llm`. It is also where we _bind_ our tools to the LLM and enforce function calling via `tool_choice=\"any\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gKxRe4tTBHX"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolCall, ToolMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "tools=[\n",
        "    rag_search_filter,\n",
        "    rag_search,\n",
        "    fetch_arxiv,\n",
        "    web_search,\n",
        "    final_answer\n",
        "]\n",
        "\n",
        "# define a function to transform intermediate_steps from list\n",
        "# of AgentAction to scratchpad string\n",
        "def create_scratchpad(intermediate_steps: list[AgentAction]):\n",
        "    research_steps = []\n",
        "    for i, action in enumerate(intermediate_steps):\n",
        "        if action.log != \"TBD\":\n",
        "            # this was the ToolExecution\n",
        "            research_steps.append(\n",
        "                f\"Tool: {action.tool}, input: {action.tool_input}\\n\"\n",
        "                f\"Output: {action.log}\"\n",
        "            )\n",
        "    return \"\\n---\\n\".join(research_steps)\n",
        "\n",
        "oracle = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        \"scratchpad\": lambda x: create_scratchpad(\n",
        "            intermediate_steps=x[\"intermediate_steps\"]\n",
        "        ),\n",
        "    }\n",
        "    | prompt\n",
        "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESCSupKbTuVR"
      },
      "source": [
        "Test the agent quickly to confirm it is functional:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5xMQ8ajTxFQ",
        "outputId": "0e7b08bc-9b32-425b-cf57-8255b4b6591b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_UL4qocVvucVLhykCbn1WEkaV', 'function': {'arguments': '{\"query\": \"interesting facts about dogs\"}', 'name': 'web_search'}, 'type': 'function'}, {'id': 'call_VeoMlK9MKFopRVLBXAqtnSm2', 'function': {'arguments': '{\"query\": \"interesting facts about dogs\"}', 'name': 'rag_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 541, 'total_tokens': 590}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_ce0793330f', 'finish_reason': 'stop', 'logprobs': None}, id='run-ddd60742-06ec-4b16-bf37-6d5eefa423eb-0', tool_calls=[{'name': 'web_search', 'args': {'query': 'interesting facts about dogs'}, 'id': 'call_UL4qocVvucVLhykCbn1WEkaV'}, {'name': 'rag_search', 'args': {'query': 'interesting facts about dogs'}, 'id': 'call_VeoMlK9MKFopRVLBXAqtnSm2'}], usage_metadata={'input_tokens': 541, 'output_tokens': 49, 'total_tokens': 590})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = {\n",
        "    \"input\": \"tell me something interesting about dogs\",\n",
        "    \"chat_history\": [],\n",
        "    \"intermediate_steps\": [],\n",
        "}\n",
        "out = oracle.invoke(inputs)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTwZ-7ZtZXq2"
      },
      "source": [
        "It is running but we are returning a lot of output here, we can narrow this down to what we need — ie, the chosen tool name and generated input args for the tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "u7wUMr2BUBt7",
        "outputId": "c32b2241-8657-4845-da30-78519e8b9473"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'web_search'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out.tool_calls[0][\"name\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se0M1pbnaFyi",
        "outputId": "f224b27c-357f-4b7d-d596-465f8c53f61f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'interesting facts about dogs'}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out.tool_calls[0][\"args\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbO6IMEZaK4q"
      },
      "source": [
        "We can see now that our Oracle decided to use the `web_search` tool with a `query` of `\"interesting facts about dogs\"` — a good choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6pAxC9kcY1F"
      },
      "source": [
        "## Define Nodes for Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ykjVWoa0kz"
      },
      "source": [
        "We will be passing the tool use decision to our `router` which will _route_ the output to the chosen node component to run (we define these below) based on the `out.tool_calls[0][\"name\"]` value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcRVPIuAgcoG"
      },
      "outputs": [],
      "source": [
        "def run_oracle(state: list):\n",
        "    print(\"run_oracle\")\n",
        "    print(f\"intermediate_steps: {state['intermediate_steps']}\")\n",
        "    out = oracle.invoke(state)\n",
        "    tool_name = out.tool_calls[0][\"name\"]\n",
        "    tool_args = out.tool_calls[0][\"args\"]\n",
        "    action_out = AgentAction(\n",
        "        tool=tool_name,\n",
        "        tool_input=tool_args,\n",
        "        log=\"TBD\"\n",
        "    )\n",
        "    return {\n",
        "        \"intermediate_steps\": [action_out]\n",
        "    }\n",
        "\n",
        "def router(state: list):\n",
        "    # return the tool name to use\n",
        "    if isinstance(state[\"intermediate_steps\"], list):\n",
        "        return state[\"intermediate_steps\"][-1].tool\n",
        "    else:\n",
        "        # if we output bad format go to final answer\n",
        "        print(\"Router invalid format\")\n",
        "        return \"final_answer\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtBEhoDOSOCQ"
      },
      "source": [
        "All of our tools can be run using the same function logic, which we define with `run_tool`. The input parameters to our tool call and the resultant output are added to our graph state's `intermediate_steps` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r27tXTBrSVLK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkxxHFwgSVIs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JGJGvDVcbvq"
      },
      "outputs": [],
      "source": [
        "tool_str_to_func = {\n",
        "    \"rag_search_filter\": rag_search_filter,\n",
        "    \"rag_search\": rag_search,\n",
        "    \"fetch_arxiv\": fetch_arxiv,\n",
        "    \"web_search\": web_search,\n",
        "    \"final_answer\": final_answer\n",
        "}\n",
        "\n",
        "def run_tool(state: list):\n",
        "    # use this as helper function so we repeat less code\n",
        "    tool_name = state[\"intermediate_steps\"][-1].tool\n",
        "    tool_args = state[\"intermediate_steps\"][-1].tool_input\n",
        "    print(f\"{tool_name}.invoke(input={tool_args})\")\n",
        "    # run tool\n",
        "    out = tool_str_to_func[tool_name].invoke(input=tool_args)\n",
        "    action_out = AgentAction(\n",
        "        tool=tool_name,\n",
        "        tool_input=tool_args,\n",
        "        log=str(out)\n",
        "    )\n",
        "    return {\"intermediate_steps\": [action_out]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwv4EsTKeZzh"
      },
      "source": [
        "## Define Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAkcBE5pebXv"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "graph.add_node(\"oracle\", run_oracle)\n",
        "graph.add_node(\"rag_search_filter\", run_tool)\n",
        "graph.add_node(\"rag_search\", run_tool)\n",
        "graph.add_node(\"fetch_arxiv\", run_tool)\n",
        "graph.add_node(\"web_search\", run_tool)\n",
        "graph.add_node(\"final_answer\", run_tool)\n",
        "\n",
        "graph.set_entry_point(\"oracle\")\n",
        "\n",
        "graph.add_conditional_edges(\n",
        "    source=\"oracle\",  # where in graph to start\n",
        "    path=router,  # function to determine which node is called\n",
        ")\n",
        "\n",
        "# create edges from each tool back to the oracle\n",
        "for tool_obj in tools:\n",
        "    if tool_obj.name != \"final_answer\":\n",
        "        graph.add_edge(tool_obj.name, \"oracle\")\n",
        "\n",
        "# if anything goes to final answer, it must then move to END\n",
        "graph.add_edge(\"final_answer\", END)\n",
        "\n",
        "app = graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "EmwkoQkthb__",
        "outputId": "f2cd4b01-b9f6-49c2-e4bf-3c235ec15aa0"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmrXAdCwiN8N"
      },
      "source": [
        "## Building Reports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPJiet2niPnQ"
      },
      "source": [
        "Let's test our research agent. First, I want to try on something simple (although not within the intended use-case of our agent):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eghr8ZBI-IJL",
        "outputId": "620075bc-51cb-444c-8cd9-9580790f0f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_oracle\n",
            "intermediate_steps: []\n",
            "rag_search.invoke(input={'query': 'interesting facts about dogs'})\n",
            "run_oracle\n",
            "intermediate_steps: [AgentAction(tool='rag_search', tool_input={'query': 'interesting facts about dogs'}, log='TBD'), AgentAction(tool='rag_search', tool_input={'query': 'interesting facts about dogs'}, log=\"Title: CALYPSO: LLMs as Dungeon Masters' Assistants\\nContent: The blink dogs are canine creatures, about the size of a large dog. Their fur is a mottled grey and white colour, and their eyes flash yellow when they use their supernatural ability to blink in and out of ex- istence. They are surprisingly fast despite their size, and can easily keep up with most horses. In terms of behavior, the blink dogs are curious but shy creatures who prefer to avoid contact with other creatures un- less absolutely necessary. If threatened, they will use their blinking power to confuse and disorientate their opponents before attacking with a pack mentality. The blink dogs of this forest seem particularly adept at working together as a unit and can prove a formidable foe if provoked.\\nArXiv ID: 2308.07540\\nRelated Papers: ['1706.03762']\\n\\n---\\nTitle: OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models\\nContent: 15 Random demonstrations  RICES  A person hanging from a telephone pole near the mountains. The brown dog is running through the grass with a yellow toy in its mouth. Demos  A trio of male musicians are performing with one playing a guitar and singing into a micro- phone, another holding a harmonica, and the third playing a bass guitar. A white dog rushes down a dirt path sur- rounded by grass and trees. Two men, both in strange hats, working over rocks in a busy urban street. The tan dog is carrying a green squeak toy in its mouth. Several people are in a group where a man in a blue shirt is smiling. A yellow dog running through a yard covered in leaves while holding a yellow toy in his mouth.\\nArXiv ID: 2308.01390\\nRelated Papers: ['1909.11059']\\n\")]\n",
            "web_search.invoke(input={'query': 'interesting facts about dogs'})\n",
            "run_oracle\n",
            "intermediate_steps: [AgentAction(tool='rag_search', tool_input={'query': 'interesting facts about dogs'}, log='TBD'), AgentAction(tool='rag_search', tool_input={'query': 'interesting facts about dogs'}, log=\"Title: CALYPSO: LLMs as Dungeon Masters' Assistants\\nContent: The blink dogs are canine creatures, about the size of a large dog. Their fur is a mottled grey and white colour, and their eyes flash yellow when they use their supernatural ability to blink in and out of ex- istence. They are surprisingly fast despite their size, and can easily keep up with most horses. In terms of behavior, the blink dogs are curious but shy creatures who prefer to avoid contact with other creatures un- less absolutely necessary. If threatened, they will use their blinking power to confuse and disorientate their opponents before attacking with a pack mentality. The blink dogs of this forest seem particularly adept at working together as a unit and can prove a formidable foe if provoked.\\nArXiv ID: 2308.07540\\nRelated Papers: ['1706.03762']\\n\\n---\\nTitle: OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models\\nContent: 15 Random demonstrations  RICES  A person hanging from a telephone pole near the mountains. The brown dog is running through the grass with a yellow toy in its mouth. Demos  A trio of male musicians are performing with one playing a guitar and singing into a micro- phone, another holding a harmonica, and the third playing a bass guitar. A white dog rushes down a dirt path sur- rounded by grass and trees. Two men, both in strange hats, working over rocks in a busy urban street. The tan dog is carrying a green squeak toy in its mouth. Several people are in a group where a man in a blue shirt is smiling. A yellow dog running through a yard covered in leaves while holding a yellow toy in his mouth.\\nArXiv ID: 2308.01390\\nRelated Papers: ['1909.11059']\\n\"), AgentAction(tool='web_search', tool_input={'query': 'interesting facts about dogs'}, log='TBD'), AgentAction(tool='web_search', tool_input={'query': 'interesting facts about dogs'}, log=\"Interesting Facts About Dogs\\nInteresting Facts About Dogs. – There are more than 150 dog breeds, divided into 8 classes: sporting, hound, working, terrier, toy, non-sporting, herding, and ...\\nhttps://www.mspca.org/pet_resources/interesting-facts-about-dogs/\\n---\\n30 Fun and Fascinating Dog Facts\\n30 Fun and Fascinating Dog Facts · 1. The Labrador Retriever has been on the AKC's top 10 most popular breeds list for longer than any other breed. · 2. A dog's ...\\nhttps://www.akc.org/expert-advice/lifestyle/dog-facts/\\n---\\nFun Dog Facts for Kids | Montecito Vets\\nHere Are 15 of the Most Interesting Fun Dog Facts For Kids · 1. Your dog can smell 40 times better than you can. · 2. Dogs are able to breathe ...\\nhttps://www.montecitopethospital.com/site/blog/2023/09/15/fun-dog-facts-kids\\n---\\nDog facts for kids!\\nDogs were the first animal domesticated (tamed) by humans, over 20,000 years ago! As they evolved from wolves, their skulls, teeth and paws shrank, and they ...\\nhttps://www.natgeokids.com/uk/discover/animals/general-animals/dog-facts/\\n---\\n23 Amazing Facts About Dogs You Probably Didn't Know\\n1. Dogs have a sense of time. It's been proven that they know the difference between a hour and five. If conditioned to, they can predict future events, such as ...\\nhttps://www.thedrakecenter.com/services/dogs/blog/23-amazing-facts-about-dogs-you-probably-didnt-know\")]\n",
            "final_answer.invoke(input={'introduction': 'Dogs are fascinating creatures that have been companions to humans for thousands of years. They are known for their loyalty, intelligence, and diverse abilities. Here are some interesting facts about dogs that highlight their unique characteristics and behaviors.', 'research_steps': '1. Conducted a specialist search on ArXiv for any research papers or articles related to interesting facts about dogs.\\n2. Performed a web search to gather general knowledge and fun facts about dogs from various reputable sources.', 'main_body': 'Dogs were the first animals to be domesticated by humans, with evidence suggesting this occurred over 20,000 years ago. As they evolved from wolves, their physical features such as skulls, teeth, and paws became smaller. Today, there are more than 150 dog breeds, categorized into eight classes: sporting, hound, working, terrier, toy, non-sporting, herding, and miscellaneous.\\n\\nOne of the most remarkable abilities of dogs is their sense of smell, which is 40 times better than that of humans. This extraordinary olfactory capability allows them to detect a wide range of scents, making them invaluable in roles such as search and rescue, detection of explosives, and even medical diagnosis.\\n\\nDogs also have a keen sense of time. Studies have shown that they can distinguish between different durations and can be conditioned to anticipate future events, such as their feeding times. This sense of time, combined with their ability to understand human emotions and commands, makes them highly trainable and responsive companions.\\n\\nAdditionally, dogs exhibit a range of behaviors that are both intriguing and endearing. For instance, they have been observed to dream, much like humans, and often exhibit twitching or paw movements during their sleep. Their social nature and pack mentality also mean that they thrive on companionship and can form strong bonds with both humans and other animals.', 'conclusion': 'Dogs are truly remarkable animals with a rich history of domestication and a wide array of unique abilities. Their exceptional sense of smell, understanding of time, and social behaviors make them not only fascinating but also invaluable companions to humans.', 'sources': '- https://www.mspca.org/pet_resources/interesting-facts-about-dogs/\\n- https://www.akc.org/expert-advice/lifestyle/dog-facts/\\n- https://www.montecitopethospital.com/site/blog/2023/09/15/fun-dog-facts-kids\\n- https://www.natgeokids.com/uk/discover/animals/general-animals/dog-facts/\\n- https://www.thedrakecenter.com/services/dogs/blog/23-amazing-facts-about-dogs-you-probably-didnt-know'})\n"
          ]
        }
      ],
      "source": [
        "out = runnable.invoke({\n",
        "    \"input\": \"tell me something interesting about dogs\",\n",
        "    \"chat_history\": [],\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjwDv5zvjACg"
      },
      "source": [
        "Let's create a function to consume the agent output and format it into our report:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZxjrfBfjFPl"
      },
      "outputs": [],
      "source": [
        "def build_report(output: dict):\n",
        "    research_steps = output[\"research_steps\"]\n",
        "    if type(research_steps) is list:\n",
        "        research_steps = \"\\n\".join([f\"- {r}\" for r in research_steps])\n",
        "    sources = output[\"sources\"]\n",
        "    if type(sources) is list:\n",
        "        sources = \"\\n\".join([f\"- {s}\" for s in sources])\n",
        "    return f\"\"\"\n",
        "INTRODUCTION\n",
        "------------\n",
        "{output[\"introduction\"]}\n",
        "\n",
        "RESEARCH STEPS\n",
        "--------------\n",
        "{research_steps}\n",
        "\n",
        "REPORT\n",
        "------\n",
        "{output[\"main_body\"]}\n",
        "\n",
        "CONCLUSION\n",
        "----------\n",
        "{output[\"conclusion\"]}\n",
        "\n",
        "SOURCES\n",
        "-------\n",
        "{sources}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjbPRhk8dhfs",
        "outputId": "bd56c2f8-b6af-44a5-9ee9-c12311ca0c50"
      },
      "outputs": [],
      "source": [
        "print(build_report(\n",
        "    output=out[\"intermediate_steps\"][-1].tool_input\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBgBR3PdiXIg"
      },
      "source": [
        "Now let's try with an on-topic question on AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a21f9VneePw_",
        "outputId": "5a8cba34-6819-46fc-fd30-cc58911f81f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_oracle\n",
            "intermediate_steps: []\n",
            "rag_search.invoke(input={'query': 'AI'})\n",
            "run_oracle\n",
            "intermediate_steps: [AgentAction(tool='rag_search', tool_input={'query': 'AI'}, log='TBD'), AgentAction(tool='rag_search', tool_input={'query': 'AI'}, log=\"Title: Cognitive Architectures for Language Agents\\nContent: AI-generated characters for supporting personalized learning and well-being. Nature Machine Intelligence, 3(12):1013â 1022, 2021. A. Peng, I. Sucholutsky, B. Li, T. R. Sumers, T. L. Griffiths, J. Andreas, and J. A. Shah. Language guided state abstractions. In Workshop on Social Intelligence in Humans and Robots at RSS 2023, 2023. E. L. Post.\\nArXiv ID: 2309.02427\\nRelated Papers: ['2305.14909']\\n\\n---\\nTitle: An In-depth Survey of Large Language Model-based Artificial Intelligence Agents\\nContent: thoughtsâ together to autonomously achieve whatever goal users set An task-driven autonomous agent leveraging GPT-4 language model, Pinecone vector search, and the LangChain framework to perform a wide range of tasks across diverse domains A developer-centric open-source framework to build, manage and run useful Autonomous AI Agents A framework allow users to configure and deploy Autonomous AI agents rapidly Table 1: LLM-based AI Agent applications. research, coding, collaboration, and general pur- pose, as shown in Tab. 1. 4.1. Chatbot Pi3 is a typical LLM-based chatting AI agent re- leased by Inflection. Like ChatGPT4 and Claude5, users can talk directly with Pi, but Pi not only serves productivity needs such as searching or an- swering questions but also focuses on emotional companionship. Pi is known for its high emotional intelligence. Users can communicate with Pi as naturally as they would with a close friend.\\nArXiv ID: 2309.14365\\nRelated Papers: ['2306.05424']\\n\")]\n",
            "fetch_arxiv.invoke(input={'arxiv_id': '2309.02427'})\n",
            "run_oracle\n",
            "intermediate_steps: [AgentAction(tool='rag_search', tool_input={'query': 'AI'}, log='TBD'), AgentAction(tool='rag_search', tool_input={'query': 'AI'}, log=\"Title: Cognitive Architectures for Language Agents\\nContent: AI-generated characters for supporting personalized learning and well-being. Nature Machine Intelligence, 3(12):1013â 1022, 2021. A. Peng, I. Sucholutsky, B. Li, T. R. Sumers, T. L. Griffiths, J. Andreas, and J. A. Shah. Language guided state abstractions. In Workshop on Social Intelligence in Humans and Robots at RSS 2023, 2023. E. L. Post.\\nArXiv ID: 2309.02427\\nRelated Papers: ['2305.14909']\\n\\n---\\nTitle: An In-depth Survey of Large Language Model-based Artificial Intelligence Agents\\nContent: thoughtsâ together to autonomously achieve whatever goal users set An task-driven autonomous agent leveraging GPT-4 language model, Pinecone vector search, and the LangChain framework to perform a wide range of tasks across diverse domains A developer-centric open-source framework to build, manage and run useful Autonomous AI Agents A framework allow users to configure and deploy Autonomous AI agents rapidly Table 1: LLM-based AI Agent applications. research, coding, collaboration, and general pur- pose, as shown in Tab. 1. 4.1. Chatbot Pi3 is a typical LLM-based chatting AI agent re- leased by Inflection. Like ChatGPT4 and Claude5, users can talk directly with Pi, but Pi not only serves productivity needs such as searching or an- swering questions but also focuses on emotional companionship. Pi is known for its high emotional intelligence. Users can communicate with Pi as naturally as they would with a close friend.\\nArXiv ID: 2309.14365\\nRelated Papers: ['2306.05424']\\n\"), AgentAction(tool='fetch_arxiv', tool_input={'arxiv_id': '2309.02427'}, log='TBD'), AgentAction(tool='fetch_arxiv', tool_input={'arxiv_id': '2309.02427'}, log=\"Recent efforts have augmented large language models (LLMs) with external\\nresources (e.g., the Internet) or internal control flows (e.g., prompt\\nchaining) for tasks requiring grounding or reasoning, leading to a new class of\\nlanguage agents. While these agents have achieved substantial empirical\\nsuccess, we lack a systematic framework to organize existing agents and plan\\nfuture developments. In this paper, we draw on the rich history of cognitive\\nscience and symbolic artificial intelligence to propose Cognitive Architectures\\nfor Language Agents (CoALA). CoALA describes a language agent with modular\\nmemory components, a structured action space to interact with internal memory\\nand external environments, and a generalized decision-making process to choose\\nactions. We use CoALA to retrospectively survey and organize a large body of\\nrecent work, and prospectively identify actionable directions towards more\\ncapable agents. Taken together, CoALA contextualizes today's language agents\\nwithin the broader history of AI and outlines a path towards language-based\\ngeneral intelligence.\")]\n",
            "web_search.invoke(input={'query': 'Artificial Intelligence overview'})\n",
            "run_oracle\n",
            "intermediate_steps: [AgentAction(tool='rag_search', tool_input={'query': 'AI'}, log='TBD'), AgentAction(tool='rag_search', tool_input={'query': 'AI'}, log=\"Title: Cognitive Architectures for Language Agents\\nContent: AI-generated characters for supporting personalized learning and well-being. Nature Machine Intelligence, 3(12):1013â 1022, 2021. A. Peng, I. Sucholutsky, B. Li, T. R. Sumers, T. L. Griffiths, J. Andreas, and J. A. Shah. Language guided state abstractions. In Workshop on Social Intelligence in Humans and Robots at RSS 2023, 2023. E. L. Post.\\nArXiv ID: 2309.02427\\nRelated Papers: ['2305.14909']\\n\\n---\\nTitle: An In-depth Survey of Large Language Model-based Artificial Intelligence Agents\\nContent: thoughtsâ together to autonomously achieve whatever goal users set An task-driven autonomous agent leveraging GPT-4 language model, Pinecone vector search, and the LangChain framework to perform a wide range of tasks across diverse domains A developer-centric open-source framework to build, manage and run useful Autonomous AI Agents A framework allow users to configure and deploy Autonomous AI agents rapidly Table 1: LLM-based AI Agent applications. research, coding, collaboration, and general pur- pose, as shown in Tab. 1. 4.1. Chatbot Pi3 is a typical LLM-based chatting AI agent re- leased by Inflection. Like ChatGPT4 and Claude5, users can talk directly with Pi, but Pi not only serves productivity needs such as searching or an- swering questions but also focuses on emotional companionship. Pi is known for its high emotional intelligence. Users can communicate with Pi as naturally as they would with a close friend.\\nArXiv ID: 2309.14365\\nRelated Papers: ['2306.05424']\\n\"), AgentAction(tool='fetch_arxiv', tool_input={'arxiv_id': '2309.02427'}, log='TBD'), AgentAction(tool='fetch_arxiv', tool_input={'arxiv_id': '2309.02427'}, log=\"Recent efforts have augmented large language models (LLMs) with external\\nresources (e.g., the Internet) or internal control flows (e.g., prompt\\nchaining) for tasks requiring grounding or reasoning, leading to a new class of\\nlanguage agents. While these agents have achieved substantial empirical\\nsuccess, we lack a systematic framework to organize existing agents and plan\\nfuture developments. In this paper, we draw on the rich history of cognitive\\nscience and symbolic artificial intelligence to propose Cognitive Architectures\\nfor Language Agents (CoALA). CoALA describes a language agent with modular\\nmemory components, a structured action space to interact with internal memory\\nand external environments, and a generalized decision-making process to choose\\nactions. We use CoALA to retrospectively survey and organize a large body of\\nrecent work, and prospectively identify actionable directions towards more\\ncapable agents. Taken together, CoALA contextualizes today's language agents\\nwithin the broader history of AI and outlines a path towards language-based\\ngeneral intelligence.\"), AgentAction(tool='web_search', tool_input={'query': 'Artificial Intelligence overview'}, log='TBD'), AgentAction(tool='web_search', tool_input={'query': 'Artificial Intelligence overview'}, log='Overview of Artificial Intelligence Technology\\nIn practice, AI is used as an umbrella term that encompasses a broad spectrum of different technologies and applications, some of which are described below.\\nhttps://www.finra.org/rules-guidance/key-topics/fintech/report/artificial-intelligence-in-the-securities-industry/overview-of-ai-tech\\n---\\nWhat is Artificial Intelligence (AI) & Why is it Important?\\nArtificial intelligence is a constellation of many different technologies working together to enable machines to sense, comprehend, act, and learn with human- ...\\nhttps://www.accenture.com/us-en/insights/artificial-intelligence-summary-index\\n---\\nWhat is Artificial Intelligence (AI)?\\nArtificial intelligence, or AI, is technology that enables computers and machines to simulate human intelligence and problem-solving capabilities. On its own or ...\\nhttps://www.ibm.com/topics/artificial-intelligence\\n---\\nWhat is Artificial Intelligence and Why It Matters in 2024?\\nArtificial Intelligence is a method of making a computer, a computer-controlled robot, or a software think intelligently like the human mind. AI ...\\nhttps://www.simplilearn.com/tutorials/artificial-intelligence-tutorial/what-is-artificial-intelligence\\n---\\nWhat is artificial intelligence (AI)? Everything you need to ...\\nArtificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. Examples of AI applications include ...\\nhttps://www.techtarget.com/searchenterpriseai/definition/AI-Artificial-Intelligence')]\n",
            "final_answer.invoke(input={'introduction': 'Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies and applications aimed at simulating human intelligence in machines. From cognitive architectures to large language models, AI is transforming various domains by enabling machines to perform tasks that typically require human intelligence.', 'research_steps': '1. Conducted a specialist search on AI using the RAG search tool.\\n2. Retrieved detailed information on specific AI papers and their abstracts.\\n3. Performed a web search to gather general knowledge and an overview of AI from various reputable sources.', 'main_body': \"Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, particularly computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using rules to reach approximate or definite conclusions), and self-correction. AI applications are numerous and diverse, ranging from natural language processing and robotics to autonomous vehicles and predictive analytics.\\n\\nOne significant area of AI research is the development of large language models (LLMs) and cognitive architectures. For instance, the paper titled 'Cognitive Architectures for Language Agents' introduces the CoALA framework, which organizes language agents with modular memory components and structured action spaces. This framework aims to contextualize current language agents within the broader history of AI and outline a path towards more capable, language-based general intelligence.\\n\\nAnother notable development is the use of AI agents that leverage advanced language models like GPT-4. These agents can perform a wide range of tasks across various domains, from research and coding to emotional companionship. For example, the chatbot Pi3 is designed not only for productivity tasks but also for providing emotional support, showcasing the versatility and potential of AI in enhancing human-machine interactions.\\n\\nThe general overview of AI highlights its importance in modern technology. AI enables machines to sense, comprehend, act, and learn, making it a cornerstone of innovations in fields such as healthcare, finance, and transportation. The continuous advancements in AI research and applications are driving significant changes in how we interact with technology and how various industries operate.\", 'conclusion': 'Artificial Intelligence is a multifaceted field that is revolutionizing the way machines interact with the world. From cognitive architectures to versatile AI agents, the advancements in AI are paving the way for more intelligent and capable systems. As research continues to evolve, AI will undoubtedly play an increasingly integral role in shaping the future of technology and society.', 'sources': '- Cognitive Architectures for Language Agents, ArXiv ID: 2309.02427\\n- An In-depth Survey of Large Language Model-based Artificial Intelligence Agents, ArXiv ID: 2309.14365\\n- Overview of Artificial Intelligence Technology, FINRA\\n- What is Artificial Intelligence (AI) & Why is it Important?, Accenture\\n- What is Artificial Intelligence (AI)?, IBM\\n- What is Artificial Intelligence and Why It Matters in 2024?, Simplilearn\\n- What is artificial intelligence (AI)? Everything you need to know, TechTarget'})\n"
          ]
        }
      ],
      "source": [
        "out = runnable.invoke({\n",
        "    \"input\": \"tell me about AI\",\n",
        "    \"chat_history\": []\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vis78CFR3puz",
        "outputId": "da622cdd-510b-4996-97c1-62c6050d66ac"
      },
      "outputs": [],
      "source": [
        "print(build_report(\n",
        "    output=out[\"intermediate_steps\"][-1].tool_input\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNyxZL4SNjhX"
      },
      "source": [
        "Let's ask about RAG specifically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0VmnWlWE6Lg",
        "outputId": "eb6cd6d7-274b-4856-b6ff-34a0dd615aab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_oracle\n",
            "intermediate_steps: []\n",
            "rag_search.invoke(input={'query': 'retrieval augmented generation'})\n",
            "run_oracle\n",
            "intermediate_steps: [AgentAction(tool='rag_search', tool_input={'query': 'retrieval augmented generation'}, log='TBD'), AgentAction(tool='rag_search', tool_input={'query': 'retrieval augmented generation'}, log=\"Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\\nContent: Due to the page limit, details of the evaluation, including case studies in three scenarios are in Appendix D. # A2: Retrieval-Augmented Code Generation and Question Answering Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic limitations of LLMs by incorporating external documents. In this section, we employ AutoGen to build a Retrieval-Augmented Generation (RAG) system (Lewis et al., 2020; Parvez et al., 2021) named Retrieval-augmented Chat. The system consists of two agents: a Retrieval-augmented User Proxy agent and a Retrieval-augmented Assistant agent, both of which are extended from built-in agents from AutoGen. The Retrieval-augmented User Proxy includes a vector database (Chroma, 5We did not evaluate ChatGPT on the whole dataset since it requires substantial manual effort and is re- stricted by its hourly message-number limitation. Multi-agent debate and LangChain ReAct were also not evaluated since they underperformed vanilla GPT-4 on the smaller test set.\\nArXiv ID: 2308.08155\\nRelated Papers: ['2103.03874']\\n\\n---\\nTitle: SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool\\nContent: 2https://www.kioxia.com/en-jp/top.html 3https://huggingface.co/ehartford/ Wizard-Vicuna-13B-Uncensored 4https://huggingface.co/intfloat/ multilingual-e5-base # 4.1 Qualitative Evaluation We compare the results of three approaches: Retrieval-Centric Generation (RCG), Retrieval- Augmented Generation (RAG), and Retrieval-OFF Generation (ROG). Note that in this work, we de- fine RAG as allowing more permissible integra- tion of LLMâ s inherent and externally retrieved knowledge, whereas RCG prioritizes clear demar- cations between context interpretation and knowl- edge memorization. Investigating advanced meth- ods in extracting RCG behavior is a promising research topic. In this work, we conduct simple experiments using prompt-engineering technique to reveal the potential of RCG approach. Specifi- cally, for RCG, we employ a retrieval suffix prompt that reads â answer the following question with the provided knowledge.â For RAG, we use a less constraining prompt that reads â answer the follow- ing question. You may use the provided knowl- edge.â\\nArXiv ID: 2308.03983\\nRelated Papers: ['2302.13971']\\n\")]\n",
            "web_search.invoke(input={'query': 'retrieval augmented generation'})\n",
            "run_oracle\n",
            "intermediate_steps: [AgentAction(tool='rag_search', tool_input={'query': 'retrieval augmented generation'}, log='TBD'), AgentAction(tool='rag_search', tool_input={'query': 'retrieval augmented generation'}, log=\"Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\\nContent: Due to the page limit, details of the evaluation, including case studies in three scenarios are in Appendix D. # A2: Retrieval-Augmented Code Generation and Question Answering Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic limitations of LLMs by incorporating external documents. In this section, we employ AutoGen to build a Retrieval-Augmented Generation (RAG) system (Lewis et al., 2020; Parvez et al., 2021) named Retrieval-augmented Chat. The system consists of two agents: a Retrieval-augmented User Proxy agent and a Retrieval-augmented Assistant agent, both of which are extended from built-in agents from AutoGen. The Retrieval-augmented User Proxy includes a vector database (Chroma, 5We did not evaluate ChatGPT on the whole dataset since it requires substantial manual effort and is re- stricted by its hourly message-number limitation. Multi-agent debate and LangChain ReAct were also not evaluated since they underperformed vanilla GPT-4 on the smaller test set.\\nArXiv ID: 2308.08155\\nRelated Papers: ['2103.03874']\\n\\n---\\nTitle: SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool\\nContent: 2https://www.kioxia.com/en-jp/top.html 3https://huggingface.co/ehartford/ Wizard-Vicuna-13B-Uncensored 4https://huggingface.co/intfloat/ multilingual-e5-base # 4.1 Qualitative Evaluation We compare the results of three approaches: Retrieval-Centric Generation (RCG), Retrieval- Augmented Generation (RAG), and Retrieval-OFF Generation (ROG). Note that in this work, we de- fine RAG as allowing more permissible integra- tion of LLMâ s inherent and externally retrieved knowledge, whereas RCG prioritizes clear demar- cations between context interpretation and knowl- edge memorization. Investigating advanced meth- ods in extracting RCG behavior is a promising research topic. In this work, we conduct simple experiments using prompt-engineering technique to reveal the potential of RCG approach. Specifi- cally, for RCG, we employ a retrieval suffix prompt that reads â answer the following question with the provided knowledge.â For RAG, we use a less constraining prompt that reads â answer the follow- ing question. You may use the provided knowl- edge.â\\nArXiv ID: 2308.03983\\nRelated Papers: ['2302.13971']\\n\"), AgentAction(tool='web_search', tool_input={'query': 'retrieval augmented generation'}, log='TBD'), AgentAction(tool='web_search', tool_input={'query': 'retrieval augmented generation'}, log='What Is Retrieval-Augmented Generation aka RAG\\nRetrieval-augmented generation (RAG) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched ...\\nhttps://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\\n---\\nWhat is RAG (Retrieval-Augmented Generation)?\\nRetrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base ...\\nhttps://aws.amazon.com/what-is/retrieval-augmented-generation/\\n---\\nWhat is retrieval-augmented generation, and what does it ...\\nA RAG system can use semantic search to retrieve relevant documents, whether from an embedding-based retrieval system, traditional database, or ...\\nhttps://github.blog/2024-04-04-what-is-retrieval-augmented-generation-and-what-does-it-do-for-generative-ai/\\n---\\nWhat is retrieval-augmented generation (RAG)?\\nRetrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external ...\\nhttps://research.ibm.com/blog/retrieval-augmented-generation-RAG')]\n",
            "final_answer.invoke(input={'introduction': 'Retrieval-Augmented Generation (RAG) is an advanced technique in the field of artificial intelligence that aims to enhance the performance of generative models by incorporating external information. This method is particularly useful for improving the accuracy and reliability of responses generated by large language models (LLMs).', 'research_steps': '1. Conducted a specialized search using the RAG search tool to gather detailed information on Retrieval-Augmented Generation.\\n2. Performed a web search to collect general knowledge and additional perspectives on RAG.\\n3. Compiled and synthesized the information from both specialized and general sources to provide a comprehensive overview.', 'main_body': 'Retrieval-Augmented Generation (RAG) is a framework designed to improve the quality of responses generated by large language models (LLMs) by integrating external information sources. Traditional generative models rely solely on the data they were trained on, which can lead to limitations in accuracy and relevance, especially when dealing with specialized or up-to-date information. RAG addresses this issue by incorporating a retrieval mechanism that fetches relevant documents or data from external databases or knowledge bases during the generation process.\\n\\nThe RAG system typically consists of two main components: a retriever and a generator. The retriever is responsible for searching and retrieving relevant documents or pieces of information based on the input query. This retrieved information is then fed into the generator, which uses it to produce a more accurate and contextually relevant response. This approach allows the model to leverage both its inherent knowledge and the most current or specialized information available externally.\\n\\nSeveral studies and implementations have demonstrated the effectiveness of RAG. For instance, the AutoGen system employs a multi-agent conversation framework to build a RAG system, enhancing its performance in tasks like code generation and question answering. Another example is the SimplyRetrieve tool, which compares different approaches, including RAG, to highlight its advantages in integrating external knowledge seamlessly.\\n\\nRAG is particularly beneficial in scenarios where the information required is dynamic or highly specialized, such as medical diagnosis, legal advice, or technical support. By grounding the generative process in authoritative and up-to-date sources, RAG significantly improves the reliability and accuracy of the generated content.', 'conclusion': 'Retrieval-Augmented Generation represents a significant advancement in the field of AI, addressing the limitations of traditional generative models by incorporating external information. This technique enhances the accuracy, relevance, and reliability of AI-generated responses, making it a valuable tool in various specialized and dynamic fields.', 'sources': '- AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation, ArXiv ID: 2308.08155\\n- SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool, ArXiv ID: 2308.03983\\n- NVIDIA Blog: What Is Retrieval-Augmented Generation aka RAG\\n- AWS: What is RAG (Retrieval-Augmented Generation)?\\n- GitHub Blog: What is retrieval-augmented generation, and what does it do for generative AI?\\n- IBM Research Blog: What is retrieval-augmented generation (RAG)?'})\n"
          ]
        }
      ],
      "source": [
        "out = runnable.invoke({\n",
        "    \"input\": \"what is retrieval augmented generation?\",\n",
        "    \"chat_history\": []\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7LssyU4Nqis",
        "outputId": "270d7a52-6eda-438b-db15-4e2b889133bd"
      },
      "outputs": [],
      "source": [
        "print(build_report(\n",
        "    output=out[\"intermediate_steps\"][-1].tool_input\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp2uaRW7cAoM"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24a571b8cc864f60bea7ae90ef331b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c52a55d3643b4f1c83bdf1b1b5fce242",
            "placeholder": "​",
            "style": "IPY_MODEL_7de73bce9c0540e2b3b2c599d57ae215",
            "value": "100%"
          }
        },
        "45580486ce0742b29d0de17850c42060": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84c25583658a4f4caeedbc19dd29457a",
            "max": 79,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9594c4de9734895862a48f90bc8c621",
            "value": 79
          }
        },
        "48ec484f11324501b1020314d4989da6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7de73bce9c0540e2b3b2c599d57ae215": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84c25583658a4f4caeedbc19dd29457a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "925f65b6b0cc47be803c021f644a8f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24a571b8cc864f60bea7ae90ef331b21",
              "IPY_MODEL_45580486ce0742b29d0de17850c42060",
              "IPY_MODEL_f1f07bc63a234590ac229d63217e197a"
            ],
            "layout": "IPY_MODEL_a6d67d04d1c8432c8840f411027a0e79"
          }
        },
        "a6d67d04d1c8432c8840f411027a0e79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9594c4de9734895862a48f90bc8c621": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bfecdc3138334cd1adcd055f338060d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c52a55d3643b4f1c83bdf1b1b5fce242": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f07bc63a234590ac229d63217e197a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48ec484f11324501b1020314d4989da6",
            "placeholder": "​",
            "style": "IPY_MODEL_bfecdc3138334cd1adcd055f338060d1",
            "value": " 79/79 [04:03&lt;00:00,  2.64s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
